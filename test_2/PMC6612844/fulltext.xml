<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6612844</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btz356</article-id><article-id pub-id-type="publisher-id">btz356</article-id><article-categories><subj-group subj-group-type="heading"><subject>Ismb/Eccb 2019 Conference Proceedings</subject><subj-group subj-group-type="category-toc-heading"><subject>Bioinformatics of Microbes and Microbiomes</subject></subj-group></subj-group></article-categories><title-group><article-title>Large scale microbiome profiling in the cloud</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Valdes</surname><given-names>Camilo</given-names></name><xref ref-type="aff" rid="btz356-aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Stebliankin</surname><given-names>Vitalii</given-names></name><xref ref-type="aff" rid="btz356-aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Narasimhan</surname><given-names>Giri</given-names></name><xref ref-type="aff" rid="btz356-aff1">1</xref><xref ref-type="aff" rid="btz356-aff2">2</xref><xref ref-type="corresp" rid="btz356-cor1"/><!--<email>giri@cs.fiu.edu</email>--></contrib></contrib-group><aff id="btz356-aff1"><label>1</label>Bioinformatics Research Group (BioRG), School of Computing and Information Sciences, Florida International University, Miami, FL, USA</aff><aff id="btz356-aff2"><label>2</label>Biomolecular Sciences Institute, Florida International University, Miami, FL, USA</aff><author-notes><corresp id="btz356-cor1">To whom correspondence should be addressed. <email>giri@cs.fiu.edu</email></corresp></author-notes><pub-date pub-type="ppub"><month>7</month><year>2019</year></pub-date><pub-date pub-type="epub" iso-8601-date="2019-07-05"><day>05</day><month>7</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>05</day><month>7</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>35</volume><issue>14</issue><fpage>i13</fpage><lpage>i22</lpage><permissions><copyright-statement>&#x000a9; The Author(s) 2019. Published by Oxford University Press.</copyright-statement><copyright-year>2019</copyright-year><license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p></license></permissions><self-uri xlink:href="btz356.pdf"/><abstract><title>Abstract</title><sec id="s1"><title>Motivation</title><p>Bacterial metagenomics profiling for metagenomic whole sequencing (mWGS) usually starts by aligning sequencing reads to a collection of reference genomes. Current profiling tools are designed to work against a small representative collection of genomes, and do not scale very well to larger reference genome collections. However, large reference genome collections are capable of providing a more complete and accurate profile of the bacterial population in a metagenomics dataset. In this paper, we discuss a scalable, efficient and affordable approach to this problem, bringing big data solutions within the reach of laboratories with modest resources.</p></sec><sec id="s2"><title>Results</title><p>We developed <sc>Flint</sc>, a metagenomics profiling pipeline that is built on top of the Apache Spark framework, and is designed for fast real-time profiling of metagenomic samples against a large collection of reference genomes. <sc>Flint</sc> takes advantage of Spark&#x02019;s built-in parallelism and streaming engine architecture to quickly map reads against a large (170 GB) reference collection of 43 552 bacterial genomes from Ensembl. <sc>Flint</sc> runs on Amazon&#x02019;s Elastic MapReduce service, and is able to profile 1 million Illumina paired-end reads against over 40&#x02009;K genomes on 64 machines in 67&#x02009;s&#x02014;an order of magnitude faster than the state of the art, while using a much larger reference collection. Streaming the sequencing reads allows this approach to sustain mapping rates of 55 million reads per hour, at an hourly cluster cost of $8.00 USD, while avoiding the necessity of storing large quantities of intermediate alignments.</p></sec><sec id="s3"><title>Availability and implementation</title><p>
<sc>Flint</sc> is open source software, available under the MIT License (MIT). Source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/camilo-v/flint">https://github.com/camilo-v/flint</ext-link>.</p></sec><sec id="s4"><title>Supplementary information</title><p>
<xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p></sec></abstract><funding-group><award-group award-type="grant"><funding-source><named-content content-type="funder-name">National Institute of Health</named-content></funding-source><award-id>1R15AI128714-01</award-id></award-group><award-group award-type="grant"><funding-source><named-content content-type="funder-name">Department of Defense</named-content><named-content content-type="funder-identifier">10.13039/100000005</named-content></funding-source><award-id>W911NF-16-1-0494</award-id></award-group><award-group award-type="grant"><funding-source><named-content content-type="funder-name">National Institute of Justice</named-content><named-content content-type="funder-identifier">10.13039/100005289</named-content></funding-source><award-id>582 2017-NE-BX-0001</award-id></award-group></funding-group><counts><page-count count="10"/></counts></article-meta></front><body><sec><title>1 Introduction and background</title><p>Microbes are ubiquitous and a microbiome is a collection of microbes that inhabit a particular environmental niche such as the human body, earth soil and the water in oceans and lakes. Metagenomics is the study of the combined genetic material found in microbiome samples, and it serves as an instrument for studying microbial biodiversities and their relationships to humans. Profiling a microbiome is a critical task that tells us what microorganisms are present, and in what proportions; this is particularly important as many human diseases are linked to changes in human microbiome composition (<xref rid="btz356-B14" ref-type="bibr">Haiser <italic>et al.</italic>, 2013</xref>; <xref rid="btz356-B18" ref-type="bibr">Koeth <italic>et al.</italic>, 2013</xref>; <xref rid="btz356-B41" ref-type="bibr">Wu and Lewis, 2013</xref>; <xref rid="btz356-B43" ref-type="bibr">Zhang <italic>et al.</italic>, 2015</xref>), and large research projects have started to investigate the relationships between the two (<xref rid="btz356-B34" ref-type="bibr">The Integrative HMP iHMP Research Network Consortium, 2014</xref>).</p><p>A powerful tool for profiling microbiomes is high-throughput DNA sequencing (<xref rid="btz356-B25" ref-type="bibr">Metzker, 2010</xref>), and whole metagenome sequencing experiments generate data that give us a lens through which we can study and profile microbiomes at a higher resolution than 16S amplicon-based sequencing analyses (<xref rid="btz356-B27" ref-type="bibr">Ranjan <italic>et al.</italic>, 2016</xref>).</p><p>Advances in sequencing technologies have steadily reduced the cost of sequencing and have led to an ever increasing number of extremely large and complex metagenomic <xref rid="btz356-B4" ref-type="bibr">datasets (Ansorge, 2009</xref>; <xref rid="btz356-B7" ref-type="bibr">Caporaso <italic>et al.</italic>, 2012</xref>). The resulting computational challenge is the production of even larger intermediate results, and need for large indexes of the reference genome collections (<xref rid="btz356-B38" ref-type="bibr">Vernikos <italic>et al.</italic>, 2015</xref>), making it impossible to process on commodity workstations or laptops. Powerful multi-user servers and clusters are an option, but the cost of higher processor speeds, greater storage volumes and huge memory sizes are out of reach for small laboratories.</p><p>To deal with the barrage of sequencing data, distributed cloud computing platforms and frameworks such as Amazon Web Services (<xref rid="btz356-B3" ref-type="bibr">Amazon.com Inc., Amazon Web Services, 2018</xref>), Apache Hadoop (<xref rid="btz356-B5" ref-type="bibr">Apache Hadoop, 2018</xref>) and Apache Spark (<xref rid="btz356-B6" ref-type="bibr">Apache Spark, 2018</xref>) have been used by researchers by taking advantage of parallel computation and economies of scale: large sequencing workloads are distributed in a cloud cluster that is comprised of many cheap, off-the-shelf compute nodes. These cloud-based solutions have been successfully used for human genomics (<xref rid="btz356-B19" ref-type="bibr">Langmead <italic>et al.</italic>, 2009a</xref>), transcriptomics (<xref rid="btz356-B28" ref-type="bibr">Roberts <italic>et al.</italic>, 2013</xref>) and more recently for metagenomics applications (<xref rid="btz356-B15" ref-type="bibr">Huang <italic>et al.</italic>, 2018</xref>; <xref rid="btz356-B45" ref-type="bibr">Zhou <italic>et al.</italic>, 2017</xref>).</p><p>Standard genomics and transcriptomics analyses for sequencing datasets usually begin by aligning sequencing reads to a reference genome (<xref rid="btz356-B36" ref-type="bibr">Trapnell and Salzberg, 2009</xref>; <xref rid="btz356-B39" ref-type="bibr">Wang <italic>et al.</italic>, 2009</xref>), and producing abundance counts (<xref rid="btz356-B35" ref-type="bibr">Trapnell <italic>et al.</italic>, 2010</xref>); but in metagenomic analyses, the alignment step is performed against a collection of reference genomes that can be extremely large, slowing down the entire operation. The MapReduce model (<xref rid="btz356-B9" ref-type="bibr">Dean and Ghemawat, 2008</xref>) along with the Spark framework have been popular in speeding up these crucial steps in the analysis of single-organism sequencing datasets, as researchers have framed the read-alignment and quantification tasks in terms of <italic>map</italic> and <italic>reduce</italic> operations: Langmead <italic>et al.</italic> used it to align human sequencing reads using the Bowtie read-mapping utility (<xref rid="btz356-B20" ref-type="bibr">Langmead <italic>et al.</italic>, 2009b</xref>) and searching for single nucleotide polymorphisms (SNPs); while <xref rid="btz356-B28" ref-type="bibr">Roberts <italic>et al.</italic> (2013</xref>) used it to speed up the quantification of human gene transcripts by the expectation-maximization (EM) algorithm.</p></sec><sec><title>2 Approach</title><sec><title>2.1 Spark and MapReduce</title><p>The MapReduce model was originally developed by Google (<xref rid="btz356-B9" ref-type="bibr">Dean and Ghemawat, 2008</xref>), and most notably popularized by the <xref rid="btz356-B5" ref-type="bibr">Apache Hadoop (2018</xref>) open-source project from the Apache foundation (<xref rid="btz356-B33" ref-type="bibr">The Apache Software Foundation, 2018</xref>). The <xref rid="btz356-B6" ref-type="bibr">Apache Spark (2018</xref>) project further expanded the Hadoop project, and introduced new optimizations for calculation speeds, and programming paradigms (<xref rid="btz356-B42" ref-type="bibr">Zaharia <italic>et al.</italic>, 2012</xref>). The MapReduce model abstracts away much of the boiler-plate programming details of developing distributable applications, and frees scientists and developers to focus their work on other critical, domain-specific, areas. The model is composed of two distinct steps: the <italic>map()</italic> step, and the <italic>reduce()</italic> step. Hadoop and Spark offer basic functions that can be used as the building blocks of a distributed computing model: the <italic>map()</italic> function takes as input a pair of parameters that make up a tuple consisting of a key and a value; while the <italic>reduce()</italic> function merges the output of the <italic>map()</italic> function by coalescing tuples with the same key.</p><p>The MapReduce model, and the Spark framework in particular, have been employed in many DNA sequencing workflows for a number of years now (<xref rid="btz356-B8" ref-type="bibr">Cattaneo <italic>et al.</italic>, 2016</xref>; <xref rid="btz356-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2018</xref>). The Crossbow project (<xref rid="btz356-B19" ref-type="bibr">Langmead <italic>et al.</italic>, 2009a</xref>) from 2009 used Spark&#x02019;s MapReduce implementation to identify Single Nucleotide Polymorphisms (SNPs) in human samples; eXpress-D (<xref rid="btz356-B28" ref-type="bibr">Roberts <italic>et al.</italic>, 2013</xref>) also used Spark to implement the expectation maximization (EM) algorithm for ambiguous DNA-fragment assignment. Spark has also been used in metagenomic analyses (<xref rid="btz356-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2018</xref>) for mapping sequencing reads against small reference databases and for clustering metagenomes (<xref rid="btz356-B29" ref-type="bibr">Rasheed and Rangwala, 2013</xref>).</p><p>A natural approach to use the Spark framework for the analysis of mWGS datasets is to partition the input of reads into smaller subsets of reads to be processed by worker nodes in a Spark cluster. This strategy works well when the dataset of reads is large. The limitation of this strategy is that it does not scale to large collections of reference genomes because a data structure (index) of the reference collection of genomes must either be duplicated in each of the worker nodes, or multiple passes of the input can be used. Indexes built from large reference collections using a k-mer based strategy are often too large to be accommodated on a single commodity machine on the cloud (<xref rid="btz356-B26" ref-type="bibr">Nasko <italic>et al.</italic>, 2018</xref>). Fast k-mer based profiling strategies have been used for profiling of mWGS datasets (<xref rid="btz356-B31" ref-type="bibr">Schaeffer <italic>et al.</italic>, 2015</xref>; <xref rid="btz356-B40" ref-type="bibr">Wood and Salzberg, 2014</xref>). But they trade-off speed for enormous indexes. More recently, alternative index-building strategies have been developed to allow the use of large collections of references with k-mer based tools, albeit only at species-level resolutions (<xref rid="btz356-B44" ref-type="bibr">Zhou <italic>et al.</italic>, 2018</xref>), but were not designed for use with a cloud-based infrastructure.</p><p>Zhou <italic>et al.</italic> developed MetaSpark (<xref rid="btz356-B45" ref-type="bibr">Zhou <italic>et al.</italic>, 2017</xref>) to align metagenomic reads to reference genomes. The tool employs Spark&#x02019;s Resilient Distributed Dataset (RDD) (<xref rid="btz356-B42" ref-type="bibr">Zaharia <italic>et al.</italic>, 2012</xref>)&#x02014;the main programming abstraction for working with large datasets&#x02014;to cache reference genome and read information across worker nodes in the cluster. By using Spark&#x02019;s RDD, MetaSpark is able to align more reads than previous tools. MetaSpark was developed with two reference datasets of bacterial genomes: a 0.6 GB reference, and the larger 1.3 GB from RefSeq&#x02019;s bacterial repository. These reference sets are small compared to the 170 GB reference set of Ensembl, and because of MetaSpark&#x02019;s use of an RDD to hold its index, it is unlikely that MetaSpark can scale to use them: the contents of an RDD are limited to available memory, and large reference sets would require correspondingly large memory allocations. It is worth pointing out the RDD memory limitations of MetaSpark in aligning reads: it took 201&#x02009;min (3.35&#x02009;h) to align 1 million reads to the small 0.6 GB reference using 10 nodes (<xref rid="btz356-B45" ref-type="bibr">Zhou <italic>et al.</italic>, 2017</xref>).</p><p>SparkHit (<xref rid="btz356-B15" ref-type="bibr">Huang <italic>et al.</italic>, 2018</xref>) was developed by Huang <italic>et al.</italic> as a toolbox for scalable genomic analysis and also included the necessary optimizations for the preprocessing. SparkHit includes a metagenomic mapping utility called &#x02018;SparkHit-recruiter&#x02019; that performs much faster than MetaSpark with similar sets of reference genomes. SparkHit performs well with large dataset of reads and small reference genome sets&#x02014;the authors profiled 2.3 TB of whole genome sequencing reads against only 21 genomes in a little over an hour and a half. The limitation of SparkHit is that it builds its reference index using a k-mer strategy that does not scale to large collections of reference genomes (<xref rid="btz356-B26" ref-type="bibr">Nasko <italic>et al.</italic>, 2018</xref>), assuming that the reference database will change with each study that is analyzed. This assumption, and the method of index building, makes SparkHit unsuitable for profiling large metagenomic datasets against large collections of reference genomes.</p></sec><sec><title>2.2 Streaming techniques</title><p>In order to process the large quantities of both input metagenomic datasets, and the large collections of reference genomes to profile against, new analysis paradigms are required that take advantage of highly parallelizable cloud infrastructure, as well as real-time data streams for consuming large input datasets.</p><p>LiveKraken (<xref rid="btz356-B32" ref-type="bibr">Tausch <italic>et al.</italic>, 2018</xref>) was developed as a real-time classification tool that improves overall analysis times, and is based on the popular Kraken (<xref rid="btz356-B40" ref-type="bibr">Wood and Salzberg, 2014</xref>) method for profiling metagenomic samples in Kraken-based workflows. LiveKraken uses the same approach as the HiLive (<xref rid="btz356-B23" ref-type="bibr">Lindner <italic>et al.</italic>, 2017</xref>) real-time mapper for Illumina reads, but extends it to metagenomic datasets. LiveKraken can ingest reads directly from the sequencing instrument in illumina&#x02019;s binary basecall format (BCL) before the instrument&#x02019;s run finishes, allowing real-time profiling of metagenomic datasets. Reads are consumed as they are produced at the instrument, and the metagenomic profile produced by LiveKraken is continuously updated. LiveKraken points the way to future classification systems that use streams of data as input, but its limitation is that it uses a k-mer based reference index&#x02014;in its publication, LiveKraken was tested with an archived version of RefSeq (circa 2015) that only contained 2787 bacterial genomes. Since then, RefSeq has grown to over 50k genomes in the latest release (version 92), and creating a K-mer based index of it would require substantial computational resources.</p><p>More recently, a Spark streaming-based aligner has been developed that uses streams of data to map reads single reference genomes. The tool, StreamAligner (Rathee and Kashyap, 2018), is implemented with Spark and the Spark-streaming API, and uses novel MapReduce-based techniques to align reads to the reference genome of a single organism. Unlike other methods, it creates its own reference genome index using suffix arrays in a distributed manner that reduces index-build times, and can then be stored in memory during an analysis run. By using the Spark streaming API, StreamAligner can continuously align reads to a single reference genome without the need of storing the input reads in local storage, and although StreamAligner has high performance when using a single genome, there is no evidence if it can scale to metagenomic workflows where tens of thousands of genomes are used, and the footprint of the reference genomes are much larger than could be fit in memory.</p></sec></sec><sec><title>3 Materials and methods</title><p>A natural approach to using MapReduce for large metagenomic analyses tasks is as follows. The <italic>map</italic> step divides the task of mapping the reads against a genomic index and the <italic>reduce</italic> step collects all the hits to each genome and constructs the microbial profile of the metagenomic sample. This approach works well when the same copy of the full genomic index can be farmed out to each node in the cluster. The approach fails when the index is too large to be provided to each cluster node or the collection of reads is too large for each cluster node. Streaming the reads allows for arbitrarily large collections of reads to be processed by each cluster node. Building an index of a &#x02018;shard&#x02019; of the reference genome database and providing each cluster node with a smaller index allows for much larger reference databases to be used for mapping the reads (<xref ref-type="fig" rid="btz356-F1">Fig.&#x000a0;1</xref>).
</p><fig id="btz356-F1" orientation="portrait" position="float"><label>Fig. 1.</label><caption><p>Overview of the <sc>Flint</sc> System. Reference genomes are partitioned so that a large reference set is be distributed across a Spark cluster, and the number of partitions matches the number of worker nodes. Samples are streamed into the cluster to avoid storage overheads as shards of 250k reads. Reads are aligned to the distributed reference genomes using a double MapReduce pipeline that continually updates metagenomic profiles as samples are streamed into the cluster. Read alignments are never stored, and are processed by each worker node as soon as they are produced</p></caption><graphic xlink:href="btz356f1"/></fig><p>Our computational framework is primarily implemented using the <italic>MapReduce</italic> model (<xref rid="btz356-B9" ref-type="bibr">Dean and Ghemawat, 2008</xref>), and deployed in a cluster launched using the <italic>Elastic Map Reduce</italic> (EMR) service offered by AWS (Amazon Web Services) (<xref rid="btz356-B3" ref-type="bibr">Amazon.com Inc., Amazon Web Services, 2018</xref>). The cluster consists of multiple &#x02018;commodity&#x02019; worker machines (a computational &#x02018;worker&#x02019; <italic>node</italic>), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a &#x02018;shard&#x02019; of the reference database (<xref ref-type="fig" rid="btz356-F2">Fig.&#x000a0;2</xref>); after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time, <sc>Flint</sc> is able to align a large number of reads to a large database of reference genomes in a much more efficient manner than that achieved by using a single powerful machine.
</p><fig id="btz356-F2" orientation="portrait" position="float"><label>Fig. 2.</label><caption><p>MapReduce workflow. Metagenomic samples can be streamed in from a distributed filesystem into the cluster were they are stored in an RDD. The first Map step generates alignments through Bowtie2 and feeds its resulting pairs to the first Reduce step, which aggregates the genomes that a single reads aligns to. The second Map step generates read contributions that are used in the second Reduce step to aggregate all the read contributions for a single genome. An output abundance matrix is generated which contains the abundances for each genome</p></caption><graphic xlink:href="btz356f2"/></fig><sec><title>3.1 Cluster provisioning</title><p>A Spark (<xref rid="btz356-B6" ref-type="bibr">Apache Spark, 2018</xref>) cluster was created using the AWS Console with the following software configuration: EMR-5.7.0, Hadoop 2.8.4, Ganglia 3.7.2, Hive 2.3.3, Hue 4.2.0, Spark 2.3.1 and Pig 0.17.0 in the US East (N. Virginia) region.</p><p>The cluster is composed of homogeneous machines for both the driver node and worker nodes, and each machine is an Amazon machine instance of type c4.2xlarge. These instances contain 8 vCPUs, 15 GB of RAM, 100 GB of EBS storage and each cost on average $0.123 USD to run per hour on the &#x02018;us-east&#x02019; availability zone on the Spot (<xref rid="btz356-B10" ref-type="bibr">EC2 Spot Market, 2018</xref>) market as of this writing in January 2019. Newer instances (c5.2xlarge) are also available for use, but their availability is infrequent in large numbers, in addition to having a higher cost per hour to run.</p><p>Resilient Distributed Datasets (RDD) (<xref rid="btz356-B42" ref-type="bibr">Zaharia <italic>et al.</italic>, 2012</xref>) are robust programming abstractions that can be used to persist data across a cluster of machines. We ingest reads from datastreams in batches of 500&#x02009;000 reads that are processed by our mapreduce pipeline. Reads are consumed either directly from their location in an Amazon S3 bucket, or from a datastream source such as a Kafka or Kinesis source. An RDD of the input read stream is created in the master node that is then broadcasted out into all the worker nodes in the cluster. The input RDD of reads is partitioned into sets of reads that are each independently aligned to a reference genome partition in each of the worker nodes.</p></sec><sec><title>3.2 A &#x02018;double&#x02019; MapReduce</title><p>An obvious way to perform MapReduce for metagenomic analysis is to have the Map function produce tuples of the form <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>, for every read <italic>r</italic> that is aligned to genomes <italic>g</italic>, while the Reduce function aggregates all tuples of the form <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula> to obtain the abundance of genome <italic>g</italic> in the sample being analyzed, effectively generating output tuples of the form <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the reported abundance of genome <italic>g</italic> in the sample being analyzed.</p><p>Unfortunately, a read may align to multiple genomes. Instead of counting a hit for every genome that the read aligns to, or counting it for only one of the genomes that the read aligns to, we follow the algorithm of <xref rid="btz356-B37" ref-type="bibr">Valdes <italic>et al.</italic> (2015</xref>), which assigns fractional counts for the genomes that a read aligns to. In order to implement this, we employ a novel double MapReduce steps, thus making it a multi-stage operation. In the modified MapReduce, the Map function generates alignments in SAM format (<xref rid="btz356-B22" ref-type="bibr">Li and 1000 Genome Project Data Processing Subgroup, 2009</xref>) by dispatching a subprocess of the Bowtie2 aligner and produces tuples of the form <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>, for every read <italic>r</italic> that is aligned to genomes <italic>g</italic>. All tuples for the same read are aggregated by the first Reduce step to generate tuples of the form <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>. The second Map step generates contributions of reads for a given genome, and the second Reduce step aggregates all tuples of the form <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula> to obtain the abundance of genome <italic>g</italic> in the sample being analyzed, effectively generating output tuples of the form <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the reported abundance of genome <italic>g</italic> in the sample being analyzed obtained by aggregating all the fractional contributions of reads that map to that genome. Note that all intermediate tuples are stored in RDDs, one for each step.</p></sec><sec><title>3.3 Reference genome preparation</title><p>Before we can use the bacterial genomes in the cluster, they need to be prepared. The process entails creating a Bowtie2 index for each shard of the reference database, and specific details on this procedure can be found in Section 2.1 of the supplementary manuscript. Briefly, the reference genomes are divided into smaller partitions that are each independently indexed by Bowtie2. The index preparation step can take considerable computational resources and time with a single machine. A parallel version of the indexing system can greatly improve performance and will be completed in the next release of <sc>Flint</sc>. Once the partitions have been indexed they are then copied to an Amazon S3 (2018) bucket that serves as a staging location for the reference shards. The staging S3 bucket holds the index so that worker nodes can copy it during their provisioning step and the analysis can start; the S3 bucket is also public, and researchers can download copies of the prepared indices for their use.</p><p>It should be noted that Ensembl&#x02019;s bacterial genome collections have grown only modestly in the last couple of releases to minimize redundancy, and reference indices for new Ensembl releases can be built relatively quickly with utility scripts provided by <sc>Flint</sc>. The cost of building a partitioned reference index is only accrued the first time it is built for a cluster of a particular size, and as part of the release of the <sc>Flint</sc> project, we are making available partitioned indices of Ensembl (v.41) of sizes 48, 64, 128, 256 and 512 which should be useful for researchers employing clusters of those sizes. These indices, along with the scripts necessary to build future versions, can be found at the GitHub repository.</p><p>We currently use minimal annotations that keep track of basic attributes for each bacterial strain; these include taxonomic identifiers, assembly lengths, etc. Future releases of the software will include a more robust annotations package that will contain data on gram staining, pathogenicity and other properties.</p><p>
<sc>Flint</sc> uses a streaming model to quickly map a large number of reads to a large collection of reference bacterial genomes by using a distributed index. The Bowtie2 DNA aligner is used internally in Spark worker nodes to align reads to the local partition of the reference index, by using a MapReduce that continuously streams reads into worker nodes. Output alignments are parsed and tabulated by worker nodes, and then sent back to master node as alignment tasks finish. <sc>Flint</sc> can be deployed on any Spark cluster, as long as the necessary software dependencies are in place; the partitioned reference index for Ensembl&#x02019;s 43k genomes is made available at the <sc>Flint</sc> website, and scripts are provided as part of the provisioning step that copy the partitions into worker nodes.</p></sec></sec><sec><title>4 Results and discussion</title><sec><title>4.1 Comparison to existing tools</title><p>
<sc>Flint</sc> was evaluated by comparing abundance profiles generated with <sc>Flint</sc> to those provided by HMP and those generated by Kraken (<xref rid="btz356-B40" ref-type="bibr">Wood and Salzberg, 2014</xref>). Note that Kraken is a <italic>k</italic>-mer based algorithm to align reads to genomic sequences and is known to be one of the most accurate ones (<xref rid="btz356-B24" ref-type="bibr">McIntyre <italic>et al.</italic>, 2017</xref>).</p><p>We selected an anterior nares sample (SRS019067) with 528k reads from the Human Microbiome Project (HMP) and analyzed it with Kraken (2.0.7-beta) and <sc>Flint</sc> and compared the results to those provided by HMP in their community abundance profiles. HMP reported 36.7% aligned reads using a bacterial database of 1751 genomes, while Kraken was able to classify 36% of the reads using their RefSeq bacterial database of 14&#x02009;506 genomes; in contrast, <sc>Flint</sc> was able to align 81% of the reads using Ensembl&#x02019;s 43k bacterial genomes. The increase number of aligned reads is due to the larger number of genomes in Ensembl&#x02014;Kraken uses RefSeq&#x02019;s so-called &#x02018;complete&#x02019; bacterial genomes, while Ensembl contains many draft genomes that increases the probability for mapping a read. <sc>Flint</sc> also aligns reads with Bowtie2 directly to the bacterial strain genomes, and does not apply lowest common ancestor (LCA) assignment to reads as Kraken does, which should mitigate any database diversity influences (genus, species and strain ratios) as noted by Nasko <italic>et al.</italic> (<xref rid="btz356-B26" ref-type="bibr">Nasko <italic>et al.</italic>, 2018</xref>). As shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S4</xref>, both <sc>Flint</sc> and Kraken identify roughly the same set of genera, but at the species level, <sc>Flint</sc> identifies significantly more species.</p><p>MetaSpark (<xref rid="btz356-B45" ref-type="bibr">Zhou <italic>et al.</italic>, 2017</xref>) and SparkHit (<xref rid="btz356-B15" ref-type="bibr">Huang <italic>et al.</italic>, 2018</xref>) are spark-based methods with a cluster infrastructure similar to <sc>Flint</sc> but their lack of support for large genome references makes direct comparison impossible. MetaSpark has a 201&#x02009;min runtime for 1 million reads with 10 nodes, profiled against a 0.6 GB reference of bacterial genomes from NCBI. In comparison, <sc>Flint</sc> takes 67&#x02009;s to profile 1 million paired-end reads against Ensembl&#x02019;s 43&#x02009;552 genomes (170 GB) with 64 nodes.</p></sec><sec><title>4.2 Reference genome collections</title><p>To test the speed of our read alignment step, we downloaded a reference collection of bacterial genomes from the <xref rid="btz356-B11" ref-type="bibr">Ensembl Bacteria (2018</xref>) repository (version 41). A total of 43&#x02009;552 bacterial genomes (strain level) were downloaded in FASTA format, accounting for 4.6 million individual FASTA assembly references. The collection included reference sequences for fully assembled chromosomes and plasmids, as well as containing sequences for draft-quality supercontigs, the latter accounting for most of the reference files in the database. The Ensembl bacterial genomes (v.41) were downloaded from the public FTP site at <ext-link ext-link-type="uri" xlink:href="http://ftp.ensemblgenomes.org">ftp.ensemblgenomes.org</ext-link>. Ensembl stores the FASTA files in &#x02018;collection&#x02019; directories, and we recursively downloaded the &#x02018;dna&#x02019; directory in each of the bacterial sub-folders. In total, 4&#x02009;672&#x02009;683 FASTA files were downloaded, with a data footprint on disk of just over 170 GB, accounting for 43&#x02009;552 bacterial strains.</p><p>Creating the Bowtie2 index for the bacterial genomes is a one-time operation as the index can be reused across cluster deployments. With a 64 worker-node cluster, we created 64 reference shards, each having a size of 2.6 GB on average. The total sequential indexing time for the 64 shards was 1d 20&#x02009;h 4&#x02009;m 33&#x02009;s on a single machine, but we also used an LSF cluster (IBM Spectrum LSF., 2019) that indexed the 64 shards in parallel, and brought down the total indexing time to just over 3&#x02009;h.</p><p>Existing metagenomic profiling tools such as MetaSpark and SparkHit use an archived version of RefSeq as their reference genomes database&#x02014;MetaSpark&#x02019;s RefSeq bacterial references was for 1.3 GB of size. Given the fact that the Ensembl database used by <sc>Flint</sc> is roughly ten times larger, we looked into how a metagenomic profile could be different by looking at how many genomes are identified by using a large or small reference collection. To do this we randomly selected 1&#x02009;M reads from an HMP anterior nares sample (SRS015996) and aligned its reads using Bowtie2 to two genome reference indices: the large collection created from the 43k Ensembl bacterial genomes, and the small collection created from 5591 bacterial representative and reference genomes from NCBI&#x02019;s Genomes (RepNG). We investigated how many clades are identified by both references, and <xref ref-type="fig" rid="btz356-F3">Figure&#x000a0;3</xref> displays the results. <xref ref-type="fig" rid="btz356-F3">Figure&#x000a0;3</xref> shows a phylogenetic tree [created with the Interactive Tree Of Life (iTOL) visualization tool (<xref rid="btz356-B21" ref-type="bibr">Letunic and Bork, 2016</xref>)] showing the differences in the phylogenetic diversity of the taxa identified in the anterior nares sample. Genomes are called as &#x02018;present&#x02019; by selecting only those genomes that have an average coverage greater than 80% along their genomic sequence. Nodes at the inner level of the figure represent the phylum taxonomic level, while nodes in the outer rings are at the species level. Green branches represents the clades identified by both references, blue branches represent clades identified by Ensembl, and red branches are clades identified by the RepNG reference set. Note that the number of clades identified by Ensembl at the higher Class and Genus taxonomic levels outnumber those identified when only using the RepNG subset.
</p><fig id="btz356-F3" orientation="portrait" position="float"><label>Fig. 3.</label><caption><p>Phylogenetic tree of taxa identified by Flint using 43k Ensembl bacterial genomes (blue), and 5k NCBI&#x02019;s Genomes references (red) with an input of 1&#x02009;M randomly selected reads from the HMP anterior nares sample (SRS015996). Genomes are identified if the average coverage in their genomic sequence is 80% or more</p></caption><graphic xlink:href="btz356f3"/></fig></sec><sec><title>4.3 Experimental setup</title><p>As mentioned earlier, the computational framework is primarily implemented using the <italic>MapReduce</italic> model (<xref rid="btz356-B9" ref-type="bibr">Dean and Ghemawat, 2008</xref>), and deployed in a cluster launched in Amazon Web Services (<xref rid="btz356-B3" ref-type="bibr">Amazon.com Inc., Amazon Web Services, 2018</xref>) <italic>Elastic Map Reduce</italic> (EMR) service. The cluster consists of multiple worker machines (i.e. a computational &#x02018;worker&#x02019; <italic>node</italic>), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a shard of the reference database; after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time, we are able to align millions of reads to the over 43k reference genomes in a much more efficient manner than either using only a single machine with considerable computational resources, or using other parallel computation approaches. Benchmarking tests were performed in Spark clusters of size 48, 64 and 128 worker nodes, all deployed in Amazon&#x02019;s EMR service for very low costs.</p></sec><sec><title>4.4 Measuring accuracy using simulated datasets</title><p>To get a measure of the accuracy of <sc>Flint&#x02019;</sc>s read-alignment pipeline, and to test the robustness of the streaming infrastructure, we simulated synthetic Illumina reads using the InSilicoSeq (<xref rid="btz356-B12" ref-type="bibr">Gourl&#x000e9; <italic>et al.</italic>, 2018</xref>) metagenomic simulator. We created three replicate dataset groups to test the accuracy of the overall pipeline, and to verify that the streaming system would not introduce any duplicate artifacts, or that the reduce steps in the Spark cluster would not exclude any of the output alignments. Each replicate group consists of 12 datasets ranging from a dataset with 1 read to a dataset with 1 million reads, created with a log-normal abundance profile, and using the default error model for the HiSeq sequencing instrument available in InSilicoSeq. Specific details on the simulation protocol, cluster configuration and detailed results for each replicate set are available in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>.</p><p>
<xref rid="btz356-T1" ref-type="table">Table&#x000a0;1</xref> outlines the results for the synthetic HiSeq datasets. Dataset evaluations were performed on a 64 worker-node cluster in AWS, with each worker node containing 8 vCPUs and 15 GB of memory. <sc>Flint</sc> achieves good performance with the HiSeq dataset achieving 99% sensitivity across all three HiSeq replicates. Alignment times on the 64 node Spark cluster using the database of over 43k Ensembl bacterial genomes show that 1 million reads are aligned in just over 1&#x02009;min with no loss of sensitivity. The &#x02018;Alignments&#x02019; column contains the number of alignments that are produced as output for each dataset&#x02014;these output alignments are not stored by the system, but rather they are processed as soon as they are generated by the worker nodes in the cluster.</p><table-wrap id="btz356-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>HiSeq synthetic datasets</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Reads</th><th rowspan="1" colspan="1">Alignments</th><th rowspan="1" colspan="1">Time</th><th rowspan="1" colspan="1">Alignment rate (%)</th><th rowspan="1" colspan="1">% Sensitivity</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">2 s 344 ms</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">23</td><td rowspan="1" colspan="1">2 s 400 ms</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">172</td><td rowspan="1" colspan="1">2 s 376 ms</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">1000</td><td rowspan="1" colspan="1">1356</td><td rowspan="1" colspan="1">2 s 455 ms</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">5000</td><td rowspan="1" colspan="1">8592</td><td rowspan="1" colspan="1">2 s 517 ms</td><td rowspan="1" colspan="1">90</td><td rowspan="1" colspan="1">98</td></tr><tr><td rowspan="1" colspan="1">10 000</td><td rowspan="1" colspan="1">23 791</td><td rowspan="1" colspan="1">3 s 193 ms</td><td rowspan="1" colspan="1">94</td><td rowspan="1" colspan="1">99</td></tr><tr><td rowspan="1" colspan="1">50 000</td><td rowspan="1" colspan="1">74 543</td><td rowspan="1" colspan="1">5 s 138 ms</td><td rowspan="1" colspan="1">96</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">100 000</td><td rowspan="1" colspan="1">103 835</td><td rowspan="1" colspan="1">8 s 320 ms</td><td rowspan="1" colspan="1">93</td><td rowspan="1" colspan="1">99</td></tr><tr><td rowspan="1" colspan="1">250 000</td><td rowspan="1" colspan="1">187 349</td><td rowspan="1" colspan="1">15 s 788 ms</td><td rowspan="1" colspan="1">95</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">500 000</td><td rowspan="1" colspan="1">275 917</td><td rowspan="1" colspan="1">29 s 18 ms</td><td rowspan="1" colspan="1">93</td><td rowspan="1" colspan="1">97</td></tr><tr><td rowspan="1" colspan="1">750 000</td><td rowspan="1" colspan="1">513 954</td><td rowspan="1" colspan="1">45 s 91 ms</td><td rowspan="1" colspan="1">95</td><td rowspan="1" colspan="1">99</td></tr><tr><td rowspan="1" colspan="1">1M</td><td rowspan="1" colspan="1">617 933</td><td rowspan="1" colspan="1">1 m 14 s 713 ms</td><td rowspan="1" colspan="1">96</td><td rowspan="1" colspan="1">99</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p>
<italic>Note</italic>: Average alignment times and alignment rates for three synthetic datasets aligned against Ensembl&#x02019;s 43k bacterial genomes. Sensitivity is the proportion of paired-end reads that were mapped correctly to the genome from which they were generated. Evaluations were performed on a 64 worker-node Spark cluster.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>4.5 Human metagenomic samples</title><p>After verifying the performance of the <sc>Flint</sc> system on simulated datasets, we tested the capabilities of the system on real metagenomic samples from the Human Microbiome Project (HMP) (<xref rid="btz356-B16" ref-type="bibr">Human Microbiome Project Consortium, 2012</xref>), which was generated using an Illumina-based sequencing system. We therefore expected a comparable performance with the HMP data as with the synthetic dataset.</p></sec><sec><title>4.6 Cluster benchmarks</title><p>Before testing the system with full human metagenomic samples, we ran a benchmark of randomly sampled paired-end reads from a HMP anterior nares sample (SRS015996) to confirm our previous observations on the synthetic datasets. Each of these read datasets was then processed through the <sc>Flint</sc> system running on a 64 worker-node cluster in AWS. <xref rid="btz356-T2" ref-type="table">Table&#x000a0;2</xref> presents the runtimes for each of the datasets, and <sc>Flint</sc> can process 1 million reads in about 67&#x02009;s.</p><table-wrap id="btz356-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Initial cluster benchmarks</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Paired-end reads</th><th rowspan="1" colspan="1">Alignments</th><th rowspan="1" colspan="1">Time (ms)</th><th rowspan="1" colspan="1">Memory (GB)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">2 s 320 ms</td><td rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">2 s 422 ms</td><td rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">902</td><td rowspan="1" colspan="1">2 s 336 ms</td><td rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1">1000</td><td rowspan="1" colspan="1">9252</td><td rowspan="1" colspan="1">2 s 316 ms</td><td rowspan="1" colspan="1">4.3</td></tr><tr><td rowspan="1" colspan="1">5000</td><td rowspan="1" colspan="1">53 918</td><td rowspan="1" colspan="1">2 s 455 ms</td><td rowspan="1" colspan="1">4.5</td></tr><tr><td rowspan="1" colspan="1">10 000</td><td rowspan="1" colspan="1">106 160</td><td rowspan="1" colspan="1">2 s 700 ms</td><td rowspan="1" colspan="1">4.9</td></tr><tr><td rowspan="1" colspan="1">50 000</td><td rowspan="1" colspan="1">538 594</td><td rowspan="1" colspan="1">5 s 437 ms</td><td rowspan="1" colspan="1">5.2</td></tr><tr><td rowspan="1" colspan="1">100 000</td><td rowspan="1" colspan="1">1 006 122</td><td rowspan="1" colspan="1">8 s 318 ms</td><td rowspan="1" colspan="1">5.8</td></tr><tr><td rowspan="1" colspan="1">250 000</td><td rowspan="1" colspan="1">2 349 518</td><td rowspan="1" colspan="1">17 s 164 ms</td><td rowspan="1" colspan="1">6.4</td></tr><tr><td rowspan="1" colspan="1">500 000</td><td rowspan="1" colspan="1">5 327 040</td><td rowspan="1" colspan="1">33 s 950 ms</td><td rowspan="1" colspan="1">7.6</td></tr><tr><td rowspan="1" colspan="1">750 000</td><td rowspan="1" colspan="1">8 439 356</td><td rowspan="1" colspan="1">50 s 880 ms</td><td rowspan="1" colspan="1">9.5</td></tr><tr><td rowspan="1" colspan="1">1M</td><td rowspan="1" colspan="1">10 710 420</td><td rowspan="1" colspan="1">1 m 7 s 609 ms</td><td rowspan="1" colspan="1">10.3</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p>
<italic>Note</italic>: Average alignment times in a 64 worker-node cluster for a set of randomly selected reads from a HMP anterior nares sample. The number of alignments column contains the output alignments that are generated by each set of reads; these alignments are processed as soon as they are produced and are not stored, therefore minimizing the local storage requirements necessary for profiling metagenomic samples.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>4.7 Full human samples</title><p>We analyzed 173 million paired-end reads from three HMP samples sequenced from anterior nares (SRS019067, 528k reads), stool (SRS065504, 116&#x02009;M reads), and supragingival plaque (SRS017511, 56&#x02009;M reads). These paired-end reads represent samples with varying levels of metagenomic diversity. For the purposes of analysis and the comparison of our execution pipeline, we created diversity classes defined by the number of unique genera present in each sample. To obtain our diversity classes, we analyzed 753 HMP samples for their abundance profiles and surveyed the number of unique genera as reported in the community abundance profiles provided by HMP (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref> for details); we then selected representative samples that contained 133 unique genera (high diversity class), 60 unique genera (medium diversity class) and 8 unique genera (low diversity class). The reasoning for using these samples was to test the performance of the <sc>Flint</sc> system in samples with varying degrees of metagenomic diversity. We speculated that low diversity samples would contain reads from a relatively small number of organisms, and therefore the alignment system would not spend too much time finding their genomes of origin. In contrast, the high diversity samples would contain reads from a large number of organisms, and the alignment system would spend more time and resources locating their origins.</p><p>
<xref rid="btz356-T3" ref-type="table">Table&#x000a0;3</xref> contains the results from running the three samples through the <sc>Flint</sc> system. The sample with the biggest number of paired-end reads, sample SRS065504 with 116 million paired-end reads, was profiled against Ensembl&#x02019;s 43k genomes in about 105&#x02009;min. The sample with the second largest number of paired-end reads, i.e. sample SRS017511 with 56 million paired-end reads, was profiled against the 43k genomes in about 94&#x02009;min; while the sample with the lowest number of paired-end reads was profiled in 53&#x02009;s. Note that the sample with 116 million paired-end reads was processed in about 10&#x02009;min more than the sample with 56 million paired-end reads&#x02014;this sample with 56 million reads is the sample that contains the highest number of unique genera (highest metagenomic diversity, 133 versus 60 in the larger sample). Since more alignments were found, the reads required more time to be processed.</p><table-wrap id="btz356-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>HMP sample analysis</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="center" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Diversity class</th><th rowspan="1" colspan="1">Unique genera</th><th rowspan="1" colspan="1">Sample ID</th><th rowspan="1" colspan="1">Paired-end reads</th><th rowspan="1" colspan="1">Alignment execution time</th><th rowspan="1" colspan="1">Streamed shards</th><th rowspan="1" colspan="1">Avg. alignments per stream shard</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Low</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">SRS019067</td><td rowspan="1" colspan="1">528 988</td><td rowspan="1" colspan="1">0 h 0 m 53 s</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">1 763 227</td></tr><tr><td rowspan="1" colspan="1">Medium</td><td rowspan="1" colspan="1">60</td><td rowspan="1" colspan="1">SRS065504</td><td rowspan="1" colspan="1">116 734 970</td><td rowspan="1" colspan="1">1 h 45 m 30 s</td><td rowspan="1" colspan="1">234</td><td rowspan="1" colspan="1">1 471 036</td></tr><tr><td rowspan="1" colspan="1">High</td><td rowspan="1" colspan="1">133</td><td rowspan="1" colspan="1">SRS017511</td><td rowspan="1" colspan="1">56 085 526</td><td rowspan="1" colspan="1">1 h 34 m 51 s</td><td rowspan="1" colspan="1">113</td><td rowspan="1" colspan="1">1 535 626</td></tr></tbody></table><table-wrap-foot><fn id="tblfn3"><p>
<italic>Note</italic>: Diversity classes were established based on the number of unique genera in 753 HMP samples. Three samples were selected from each diversity class and analyzed in a 64 worker-node cluster. Alignment execution time measures the total time to align all the sample reads against Ensembl&#x02019;s 43k bacterial genomes. The streamed shards are the number of 250k read sets that are streamed into the cluster, and the average alignment per shard is the average number of alignments produced by each shard.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>4.8 Streaming performance</title><p>The samples in <xref rid="btz356-T3" ref-type="table">Table&#x000a0;3</xref> were streamed into the cluster through Spark&#x02019;s streaming engine. The entire sample is never ingested all at once, but rather, we stream in shards of each sample so that we do not overrun the cluster with so much data that it would cause a cluster failure. To find the ideal number of reads that we could use a size of a stream shard, we looked at the results in <xref rid="btz356-T2" ref-type="table">Table&#x000a0;2</xref> and <xref ref-type="fig" rid="btz356-F4">Figure&#x000a0;4</xref>. <xref ref-type="fig" rid="btz356-F4">Figure&#x000a0;4B</xref> displays a logarithmic curve of the alignment times for all 12 sizes of the paired-end read datasets, and while we can align 1 million reads in about 67&#x02009;s, doing so creates so many alignments that each of the Spark executor processes running in each worker node could run out of memory. We looked for the &#x02018;knee-in-the-curve&#x02019; in <xref ref-type="fig" rid="btz356-F4">Figure&#x000a0;4B</xref>, marked by the vertical magenta line, and identified a size of 250k paired-end reads as a good trade-off between shard size and cluster performance. When we analyzed the three HMP samples in <xref rid="btz356-T3" ref-type="table">Table&#x000a0;3</xref> we set the streaming shard size to 250k reads, and 2 shards were created for the anterior nares sample (low diversity, 500&#x02009;k reads), 234 shards were created for the stool sample (medium diversity, 116&#x02009;M reads) and 113 shards for the supragingival plaque sample (high diversity, 56&#x02009;M reads).
</p><fig id="btz356-F4" orientation="portrait" position="float"><label>Fig. 4.</label><caption><p>Initial Benchmarks. (<bold>A</bold>) The running time for 12 paired-end read datasets in a 64 worker-node cluster. These 12 datasets were used to estimate the optimal number of reads that a 64 worker-node cluster could handle without any memory pressure, or network issues. Note that while 1 million paired-end reads can be mapped in 67&#x02009;s against 43k bacterial strains, it is not ideal as the cluster&#x02019;s memory is overwhelm with alignments. (<bold>B</bold>) The logarithmic running time of the 12 datasets, and the 250k paired-end read dataset was chosen as a good trade-off between speed and resource availability</p></caption><graphic xlink:href="btz356f4"/></fig></sec><sec><title>4.9 Cloud costs</title><p>All experiments were conducted in Amazon&#x02019;s Elastic MapReduce service (EMR) (<xref rid="btz356-B1" ref-type="bibr">Amazon EMR, 2018</xref>) and used the &#x02018;c4.2xlarge&#x02019; machine instance type. These machines contain 8 vCPUs, 15 GB of RAM and 100 GB of EBS storage; at the time of the experimental runs, each machine cost $0.123 USD in the Amazon&#x02019;s Spot market (<xref rid="btz356-B10" ref-type="bibr">EC2 Spot Market, 2018</xref>). All results reported here were obtained on a cluster of 65 total machines (64 worker-nodes, 1 master node) with a cost of $0.123 USD per node, for an overall cluster cost of $8.00 per hour.</p></sec></sec><sec><title>5 Conclusion</title><p>In this work we have shown how large metagenomic samples comprising millions of paired-end reads can be profiled against a large collection of reference bacterial genomes in a fast and economical way. Our implementation relies on the freely available Spark framework to distribute the alignment of millions of sequencing reads against Ensembl&#x02019;s collection of 43k bacterial genomes. The reference genomes are partitioned in order to distribute the genome sequences across worker machines, and this allows us to use large collections of reference sequences. By using the well-known Bowtie2 aligner under the hood in the worker-nodes, we are able to maintain fast alignment rates, without loss of accuracy.</p><p>To date, profiling metagenomic samples against thousands of reference genomes has not been possible for research groups with access to modest computing resources. This is due to the size of the reference genomes and the financial costs of the computing resources necessary to employ them. By using distributed frameworks such as Spark, along with affordable cloud computing services such as Amazon&#x02019;s EMR, we are able to distribute a large collection of reference genomes (totaling 170 GB of reference sequence, and 4.6 million assembly FASTA files) and use a MapReduce strategy to profile millions of metagenomic sequencing reads against them in a matter of hours, and at minimal financial costs, thus bringing sophisticated metagenomic analyses within reach of small research groups with modest resources.</p><p>
<sc>Flint</sc> is open source software written in Python and available under the MIT License (MIT). The source code can be obtained at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/camilo-v/flint">https://github.com/camilo-v/flint</ext-link>. The repository includes instructions and documentation on provisioning an EMR cluster, deploying the necessary partitioned reference genome indices into worker nodes, and launching an analysis job. <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>, simulation datasets and partitioned reference indices can be found in the <sc>Flint</sc> project website at <ext-link ext-link-type="uri" xlink:href="http://biorg.cs.fiu.edu/">http://biorg.cs.fiu.edu/</ext-link>.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>btz356_Supplementary_Data</label><media xlink:href="btz356_supplementary_data.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgments</title><p>The authors would like to thank Eric S. Johnson and John Flynn at the Computer Science Department&#x02019;s IT Support group for their help with managing the reference genomes datasets, and to the members of the Bioinformatics Research Group (BioRG) for valuable feedback on the project. Also helpful in obtaining initial results was The High Performance Group (HPC) group at FIU&#x02019;s Division of Information Technology.</p><sec><title>Funding</title><p>This work was supported in part by Amazon&#x02019;s &#x02018;AWS Cloud Credits for Research&#x02019; program, awarded to CV. The work of GN was supported by National Institute of Health (award 580 number 1R15AI128714-01), Department of Defense (contract number 581 W911NF-16-1-0494) and National Institute of Justice (award number 582 2017-NE-BX-0001).</p><p>
<italic>Conflict of Interest</italic>: none declared.</p></sec></ack><ref-list id="ref1"><title>References</title><ref id="btz356-B1"><mixed-citation publication-type="other">Amazon Elastic MapReduce, EMR. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/emr">https://aws.amazon.com/emr</ext-link> (16 November 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B2"><mixed-citation publication-type="other">Amazon Simple Storage Service, S3. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/s3">https://aws.amazon.com/s3</ext-link> (16 November 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B3"><mixed-citation publication-type="other">Amazon Web Services, AWS. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/">https://aws.amazon.com/</ext-link> (17 October 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B4"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ansorge</surname><given-names>W.J.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Next-generation DNA sequencing techniques</article-title>. <source>New Biotechnol</source>., <volume>25</volume>, <fpage>195</fpage>&#x02013;<lpage>203</lpage>.</mixed-citation></ref><ref id="btz356-B5"><mixed-citation publication-type="other">Apache Hadoop. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="http://hadoop.apache.org">http://hadoop.apache.org</ext-link> (17 October 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B6"><mixed-citation publication-type="other">Apache Spark. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="http://spark.apache.org">http://spark.apache.org</ext-link> (17 October 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Caporaso</surname><given-names>J.G.</given-names></name></person-group>
<etal>et al</etal> (<year>2012</year>) 
<article-title>Ultra-high-throughput microbial community analysis on the Illumina HiSeq and MiSeq platforms</article-title>. <source>ISME J</source>., <volume>6</volume>, <fpage>1621</fpage>&#x02013;<lpage>1624</lpage>.<pub-id pub-id-type="pmid">22402401</pub-id></mixed-citation></ref><ref id="btz356-B8"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Cattaneo</surname><given-names>G.</given-names></name></person-group>
<etal>et al</etal> (<year>2016</year>) <chapter-title>MapReduce in computational biology &#x02013; a synopsis</chapter-title> In: Federico,R. <etal>et al</etal> (eds) <source>Advances in Artificial Life, Evolutionary Computation, and Systems Chemistry</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Cham</publisher-loc>, pp. <fpage>53</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="btz356-B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Dean</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Ghemawat</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>) 
<article-title>MapReduce: simplified data processing on large clusters</article-title>. <source>Commun. ACM</source>, <volume>51</volume>, <fpage>107</fpage>&#x02013;<lpage>113</lpage>.</mixed-citation></ref><ref id="btz356-B10"><mixed-citation publication-type="other">EC2 Spot Market. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/ec2/spot">https://aws.amazon.com/ec2/spot</ext-link> (16 November 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B11"><mixed-citation publication-type="other">Ensembl Bacteria. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://bacteria.ensembl.org/index.html">https://bacteria.ensembl.org/index.html</ext-link> (15 October 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Gourl&#x000e9;</surname><given-names>H.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title> Simulating Illumina metagenomic data with InSilicoSeq</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>521</fpage>&#x02013;<lpage>522</lpage>.</mixed-citation></ref><ref id="btz356-B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>R.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>Bioinformatics applications on Apache Spark</article-title>. <source>GigaScience</source>, <volume>7</volume>, <fpage>giy098</fpage>.</mixed-citation></ref><ref id="btz356-B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Haiser</surname><given-names>H.J.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Predicting and manipulating cardiac drug inactivation by the human gut Bacterium <italic>Eggerthella lenta</italic></article-title>. <source>Science (New York, NY)</source>, <volume>341</volume>, <fpage>295</fpage>&#x02013;<lpage>298</lpage>.</mixed-citation></ref><ref id="btz356-B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>Analyzing large scale genomic data on the cloud with Sparkhit</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>34</volume>, <fpage>1457</fpage>&#x02013;<lpage>1465</lpage>.</mixed-citation></ref><ref id="btz356-B16"><mixed-citation publication-type="journal">Human Microbiome Project Consortium (<year>2012</year>) 
<article-title>A framework for human microbiome research</article-title>. <source>Nature</source>, <volume>486</volume>, <fpage>215</fpage>&#x02013;<lpage>221</lpage>.<pub-id pub-id-type="pmid">22699610</pub-id></mixed-citation></ref><ref id="btz356-B17"><mixed-citation publication-type="other">IBM Spectrum LSF. (<year>2019</year>) <ext-link ext-link-type="uri" xlink:href="https://www.ibm.com/support/knowledgecenter/en/SSWRJV/product_welcome_spectrum_lsf.html">https://www.ibm.com/support/knowledgecenter/en/SSWRJV/product_welcome_spectrum_lsf.html</ext-link> (20 March 2019, date last accessed).</mixed-citation></ref><ref id="btz356-B18"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Koeth</surname><given-names>R.A.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis</article-title>. <source>Nat. Med</source>., <volume>19</volume>, <fpage>576</fpage>&#x02013;<lpage>585</lpage>.<pub-id pub-id-type="pmid">23563705</pub-id></mixed-citation></ref><ref id="btz356-B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Langmead</surname><given-names>B.</given-names></name></person-group>
<etal>et al</etal> (<year>2009a</year>) 
<article-title>Searching for SNPs with cloud computing</article-title>. <source>Genome Biol</source>., <volume>10</volume>, <fpage>R134</fpage>.<pub-id pub-id-type="pmid">19930550</pub-id></mixed-citation></ref><ref id="btz356-B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Langmead</surname><given-names>B.</given-names></name></person-group>
<etal>et al</etal> (<year>2009b</year>) 
<article-title>Ultrafast and memory-efficient alignment of short DNA sequences to the human genome</article-title>. <source>Genome Biol</source>., <volume>10</volume>, <fpage>R25.</fpage><pub-id pub-id-type="pmid">19261174</pub-id></mixed-citation></ref><ref id="btz356-B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Letunic</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Bork</surname><given-names>P.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Interactive tree of life (itol) v3: an online tool for the display and annotation of phylogenetic and other trees</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>gkw290</fpage>.</mixed-citation></ref><ref id="btz356-B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group>
<etal>et al</etal> and 1000 Genome Project Data Processing Subgroup. (<year>2009</year>) 
<article-title>The Sequence Alignment/Map format and SAMtools</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>25</volume>, <fpage>2078</fpage>&#x02013;<lpage>2079</lpage>.</mixed-citation></ref><ref id="btz356-B23"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lindner</surname><given-names>M.S.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>HiLive: real-time mapping of Illumina reads while sequencing</article-title>. <source>Bioinformatics (Oxford, England</source>, <volume>33</volume>, 917&#x02013;319.</mixed-citation></ref><ref id="btz356-B24"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>McIntyre</surname><given-names>A.B.R.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Comprehensive benchmarking and ensemble approaches for metagenomic classifiers</article-title>. <source>Genome Biol</source>., <volume>18</volume>, <fpage>182</fpage>.<pub-id pub-id-type="pmid">28934964</pub-id></mixed-citation></ref><ref id="btz356-B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Metzker</surname><given-names>M.L.</given-names></name></person-group> (<year>2010</year>) 
<article-title>Sequencing technologies &#x02013; the next generation</article-title>. <source>Nat. Rev. Genet</source>., <volume>11</volume>, <fpage>31</fpage>&#x02013;<lpage>46</lpage>.<pub-id pub-id-type="pmid">19997069</pub-id></mixed-citation></ref><ref id="btz356-B26"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Nasko</surname><given-names>D.J.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>RefSeq database growth influences the accuracy of k-mer-based lowest common ancestor species identification</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>165</fpage>.<pub-id pub-id-type="pmid">30373669</pub-id></mixed-citation></ref><ref id="btz356-B27"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ranjan</surname><given-names>R.</given-names></name></person-group>
<etal>et al</etal> (<year>2016</year>) 
<article-title>Analysis of the microbiome: advantages of whole genome shotgun versus 16S amplicon sequencing</article-title>. <source>Biochem. Biophys. Res. Commun</source>., <volume>469</volume>, <fpage>967</fpage>&#x02013;<lpage>977</lpage>.<pub-id pub-id-type="pmid">26718401</pub-id></mixed-citation></ref><ref id="btz356-B28"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Roberts</surname><given-names>A.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Fragment assignment in the cloud with eXpress-D</article-title>. <source>BMC Bioinformatics</source>, <volume>14</volume>, <fpage>358</fpage>.<pub-id pub-id-type="pmid">24314033</pub-id></mixed-citation></ref><ref id="btz356-B29"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Rasheed</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Rangwala</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>) 
<article-title>A map-reduce framework for clustering metagenomes</article-title>. In: <source>2013 IEEE International Symposium on Parallel &#x00026; Distributed Processing, Workshops and Phd Forum, May 20&#x02013;24, 2013</source>. 
<publisher-name>IEEE Computer Society</publisher-name>, 
<publisher-loc>Washington, DC, USA</publisher-loc>, pp. <fpage>549</fpage>&#x02013;<lpage>558</lpage>.</mixed-citation></ref><ref id="btz356-B30"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rathee</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Kashyap</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>) 
<article-title>StreamAligner: a streaming based sequence aligner on Apache Spark</article-title>. <source>J. Big Data</source>, <volume>5</volume>, <fpage>8</fpage>.</mixed-citation></ref><ref id="btz356-B31"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Schaeffer</surname><given-names>L.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) Pseudoalignment for metagenomic read assignment. <italic>arXiv.org</italic>.</mixed-citation></ref><ref id="btz356-B32"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Tausch</surname><given-names>S.H.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>LiveKraken &#x02013; Real-time metagenomic classification of Illumina data</article-title>. <source>Bioinformatics (Oxford, England)</source>.</mixed-citation></ref><ref id="btz356-B33"><mixed-citation publication-type="other">The Apache Software Foundation. (<year>2018</year>) <ext-link ext-link-type="uri" xlink:href="https://www.apache.org">https://www.apache.org</ext-link>. (17 October 2018, date last accessed).</mixed-citation></ref><ref id="btz356-B34"><mixed-citation publication-type="journal">The Integrative HMP iHMP Research Network Consortium. (<year>2014</year>) 
<article-title>The integrative human microbiome project: dynamic analysis of microbiome-host omics profiles during periods of human health and disease</article-title>. <source>Cell Host Microbe</source>, <volume>16</volume>, <fpage>276</fpage>&#x02013;<lpage>289</lpage>.<pub-id pub-id-type="pmid">25211071</pub-id></mixed-citation></ref><ref id="btz356-B35"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Trapnell</surname><given-names>C.</given-names></name></person-group>
<etal>et al</etal> (<year>2010</year>) 
<article-title>Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation</article-title>. <source>Nat. Biotechnol</source>., <volume>28</volume>, <fpage>511</fpage>&#x02013;<lpage>515</lpage>.<pub-id pub-id-type="pmid">20436464</pub-id></mixed-citation></ref><ref id="btz356-B36"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Trapnell</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Salzberg</surname><given-names>S.L.</given-names></name></person-group> (<year>2009</year>) 
<article-title>How to map billions of short reads onto genomes</article-title>. <source>Nat. Biotechnol</source>., <volume>27</volume>, <fpage>455</fpage>&#x02013;<lpage>457</lpage>.<pub-id pub-id-type="pmid">19430453</pub-id></mixed-citation></ref><ref id="btz356-B37"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Valdes</surname><given-names>C.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Detecting bacterial genomes in a metagenomic sample using NGS reads</article-title>. <source>Stat. Interface</source>, <volume>8</volume>, <fpage>477</fpage>&#x02013;<lpage>494</lpage>.</mixed-citation></ref><ref id="btz356-B38"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Vernikos</surname><given-names>G.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Ten years of pan-genome analyses</article-title>. <source>Curr. Opin. Microbiol</source>., <volume>23</volume>, <fpage>148</fpage>&#x02013;<lpage>154</lpage>.<pub-id pub-id-type="pmid">25483351</pub-id></mixed-citation></ref><ref id="btz356-B39"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group>
<etal>et al</etal> (<year>2009</year>) 
<article-title>RNA-Seq: a revolutionary tool for transcriptomics</article-title>. <source>Nat. Rev. Genet</source>., <volume>10</volume>, <fpage>57</fpage>&#x02013;<lpage>63</lpage>.<pub-id pub-id-type="pmid">19015660</pub-id></mixed-citation></ref><ref id="btz356-B40"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wood</surname><given-names>D.E.</given-names></name>, <name name-style="western"><surname>Salzberg</surname><given-names>S.L.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Kraken: ultrafast metagenomic sequence classification using exact alignments</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>R46</fpage>.<pub-id pub-id-type="pmid">24580807</pub-id></mixed-citation></ref><ref id="btz356-B41"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>G.D.</given-names></name>, <name name-style="western"><surname>Lewis</surname><given-names>J.D.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Analysis of the human gut microbiome and association with disease</article-title>. <source>Clin. Gastroenterol. Hepatol</source>., <volume>11</volume>, <fpage>774</fpage>&#x02013;<lpage>777</lpage>.<pub-id pub-id-type="pmid">23643636</pub-id></mixed-citation></ref><ref id="btz356-B42"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Zaharia</surname><given-names>M.</given-names></name></person-group>
<etal>et al</etal> (<year>2012</year>) Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. In: <italic>9th USENIX Symposium on Networked Systems Design and Implementation</italic>, pp. <fpage>15</fpage>&#x02013;<lpage>28</lpage>.</mixed-citation></ref><ref id="btz356-B43"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Metagenomics: a new way to illustrate the crosstalk between infectious diseases and host microbiome</article-title>. <source>Int. J. Mol. Sci</source>., <volume>16</volume>, <fpage>26263</fpage>&#x02013;<lpage>26279</lpage>.<pub-id pub-id-type="pmid">26540050</pub-id></mixed-citation></ref><ref id="btz356-B44"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>ReprDB and panDB: minimalist databases with maximal microbial representation</article-title>. <source>Microbiome</source>, <volume>6</volume>, <fpage>15</fpage>.<pub-id pub-id-type="pmid">29347966</pub-id></mixed-citation></ref><ref id="btz356-B45"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>MetaSpark: a spark-based distributed processing tool to recruit metagenomic reads to reference genomes</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>33</volume>, <fpage>1090</fpage>&#x02013;<lpage>1092</lpage>.</mixed-citation></ref></ref-list></back></article>